---
title: "Assignment_2"
author: "Yilin Bai"
date: "2022-10-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### The link to my GitHub Repo

[My GitHub Repository(%5Bhttps://github.com/YilinBai-hub/R.git)](https://github.com/YilinBai-hub/R.git)

#### 1. Correct analysis of Clark data to generate the summary statistics (means, SD, N) for each of the fish species' average activity for each treatment.

**A. Libraries and Packages**

Install and load *tidyr, dplyr, plotrix, flextable, ggplot2, readr, orchaRd* package.

```{r}
library(tidyr)
library(dplyr)
library('plotrix')
library('flextable')
library(ggplot2)
library(readr)
library(orchaRd)
```

**B. Import of the Csv file**

Download the files "OA_activitydat_20190302_BIOL3207.csv" file from the course Wattle site. Import of the file into R as a tibble by using `read_csv()` and have a general impression of all the data.

```{r message=FALSE}
path <- "./Data/OA_activitydat_20190302_BIOL3207.csv"
data_activity <- read_csv(path)
data_activity
```

**C. Quality control of data to identify and address problematic entries**

There is some missing data in both `activity` and `animal_id` columns. Remove it with `filter(!is.na())`

```{r}
data_activity <- data_activity %>% filter(!is.na(activity))
data_activity <- data_activity %>% filter(!is.na(animal_id))
data_activity
```

**D. Generate the summary statistics**

Generate means, SD, N for each of the fish species' average activity for each treatment by using `group_by()` and `summarise()` function separately. And use the `merge()` function to combine the two tibbles of control and treatment.

```{r message=FALSE}
control_acticity <- data_activity %>% group_by(species) %>% filter(treatment=="control")%>% summarise(ctrl.n =n(),ctrl.mean = mean(activity),ctrl.sd = sd(activity))

co2_activity <-  data_activity %>% group_by(species) %>% filter(treatment=="CO2")%>% summarise(oa.n =n(),oa.mean = mean(activity),oa.sd = sd(activity))

group_activity <-  merge(control_acticity,co2_activity)
group_activity
```

#### 2. Through coding, merge the summary statistics generated from 1 with the metadata from Clark et al. (2020).

Import of the file into R as a tibble by using `read_csv()`.

Because there are no shared columns in the two tables, we use the `merge()` function to add the columns from both tables and generate a new table "clark_total".

```{r message=FALSE}
clark_paper_data <- read_csv("./Data/clark_paper_data.csv")
clark_total <- merge(clark_paper_data,group_activity)
clark_total
```

#### 3. Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset.

**A. Import of the file into R as a tibble by using `read_csv()` and have a general impression of all the data.**

```{r message=FALSE}
ocean_meta_data <- read_csv("./Data/ocean_meta_data.csv")
str(ocean_meta_data)
```

**B. Compare the two tibbles, there are some differences that may affect the merging of tables**

First, change the coercion of two column "Pub year IF" and "2017 IF" into character by using the function `as.character()`.

```{r}
clark_total$"Pub year IF" <- as.character(clark_total$"Pub year IF")
clark_total$"2017 IF" <- as.character(clark_total$"2017 IF")
```

Second, the "species" column names in the two tables are case-sensitive. Modify them to make them consistent.

```{r}
colnames(clark_total)[16] <- "Species" 
```

Third, I notice that in "ocean_meta_data2.csv", if there is no environmental Cue/stimulus, a "-" is used in the "Cue/stimulus type" column, whereas in "clark_total", The column "Cue/stimulus type" uses "NA", so change "NA" into "-".

```{r}
clark_total[is.na(clark_total)] <- "-"
```

**C. Merge two tables!**

```{r message=FALSE}
ocean_total <- bind_rows(clark_total,ocean_meta_data)
```

**D. Quality control of data to identify and address problematic entries**

According to the README file, the definition of activity column is **"Number of seconds the fish was active per minute, averaged across the duration of the trial."**

Therefore, the data value of the `activity` should be greater than 0, and the values of the `ctrl.mean` and `oa.mean` should also be greater than 0. Filter the data by using the `filter()` function.

```{r}
ocean_total <- ocean_total %>% filter(ctrl.mean>0) %>% filter(oa.mean>0)
```

#### 4. Correctly calculate the log response ratio (lnRR) effect size for every row of the data frame using metafor's `escalc()` function.

-   'ROM' - calculate the effect size (log response ratio of means) and sample variable of each row.

-   'm1i' and 'm2i' - the mean of treatment and control group

-   'sd1i' and 'sd2i' - the SD of treatment and control group. 'n1i' and 'n2i' - the sample size of treatment and control group.

-   Add the lnRR and sample variation as two columns named as 'lnRR', 'V_lnRR'.

```{r warning=FALSE}
ocean_total <- metafor::escalc(measure = "ROM", m1i = oa.mean, m2i = ctrl.mean, sd1i =oa.sd
, sd2i = ctrl.sd, n1i = oa.n, n2i = ctrl.n, data = ocean_total, var.names = c("lnRR", "V_lnRR"))
```

#### 5. Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor's `rma.mv()` function.

**A. Add in observation-level (`residual`) column.**

This column simply counts, from 1 to the number of rows. Add this to estimate the within study variation.

```{r}
ocean_total <- tibble::rowid_to_column(ocean_total, "residual")
```

**B. Filter out the NA values in effect size and sample variation which not contribute to the analyse.**

```{r}
ocean_total <- ocean_total %>% filter(!is.na(lnRR))
ocean_total <- ocean_total %>% filter(!is.na(V_lnRR))
```

**C. Filter out the data outliers**

-   Some sample variance are very large, which may be data outliers.

-   Because their sample variance is too large, it will lead to the instability of Multi-level meta-analytic model, which will also lead to difficulties in subsequent analysis and observation of publication bias.

-   So I filtered out the three outlier values from the data.

```{r}
ocean_total <- ocean_total %>% filter(V_lnRR<5000)
```

**D. Build the Multi-level meta-analytic model**

-   In this model,I define the effect size as `lnRR`, and the sample variance as `V_lnRR` by using the function `rma.mv()`.

-   I'm also estimating a random effect variance for `species`, between study (`study_id`) within-study (`residual`) grouping variables to control for non-independence and understand sources driving effect size variability.

```{r warning=FALSE}

# Multi-level meta-analytic model
MLMA <- metafor::rma.mv(lnRR~1, V = V_lnRR, 
                   method="REML",
                   random=list(~1|Study,
                               ~1|residual), 
                   data=ocean_total)
MLMA
```

Use the `predict()`function to transfer the lnRR(log respond ratio) back to RR (Respond ratio).

```{r,predict,fig.align='center',tab.cap = "The predict effect size and 95% confidence intervals of RR (Respond ratio)" }
pis <- predict(MLMA,transf=exp)
pis
```

**E. Use `i2_ml()`function to estimate the proportion of Total Heterogeneity:** $I_{total}^2$

```{r het}
i2_vals <- orchaRd::i2_ml(MLMA)
i2 <- tibble(type = firstup(gsub("I2_", "",names(i2_vals))), I2 = i2_vals)


flextable(i2) %>% 
    align(part = "header", align = "center") %>% 
  compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>% 
  compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")), as_b("(%)")))
```

#### 6. Written paragraph of the findings and what they mean which is supported with a figure.

**A. Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).**

I change the measure lnRR back to RR (respond ratio)

-   Overall meta-analytic mean - Estimated to be `r pis$pred`, which tells us that the mean RR value is larger than 1, indicate the mean of the control group is little higher than that of the treatment group over all studies. But it is a weak difference.

-   95% confidence intervals: `r pis[[3]]` to `r pis[[4]]`. We have 95% confident true mean of the RR value falls between `r pis[[3]]` and `r`pis[[4]]\`.

-   NULL hypothesis - After test,p value = `r MLMA$pval` \> 0.05. So we accept the NULL hypothesis (lnRR = 0). Indicate no significant difference between the mean of control group and treatment group. We can came to that conclusion that increase of acidification do not have significant effects on behavior.

**B. Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor)**

-   The total I2 = `r i2_vals[[1]]`% indicates a significant amount of heterogeneity among effects (Q = 3941, df = 122, p = \<0.001). Whereas none of the variation is driven by the sampling variance and there is lot of heterogeneity in the data. All the variation in due to the actual differeces between the study and within the study.

-   The **RR** expected to be as low as `r predict(MLMA)[[5]]` to as high as `r predict(MLMA)[[6]]`95% of the time.

-   The factors drives heterogeneity variability:

1.  Differences among studies explain `r i2_vals[[2]]`% of effect size variation.
2.  Difference within the study (observation) explain `r i2_vals[[3]]`% of the effect size variation.

**C. Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure**

```{r rochard, fig.align='center', fig.cap= "Figure 1: Forest plot showing the mean log response ratio between the treatment group and control group. k = the number of effect sizes and the number of studies are in brackets. The size of the effect is scaled by the precision of each effect size value, which is 1 / the sqrt(v_lnRR)"}

orchaRd::orchard_plot(MLMA, group = "Study", data = ocean_total, xlab = "The log response ratio(lnRR)", angle = 45,trunk.size = 2,branch.size = 10,twig.size = 2)
```

#### 7. Funnel plot for visually assessing the possibility of publication bias.

**A. Generate the Funnel plot with the function `funnel()`.**

-   `x` is the vector of effect sizes, lnRR.

-   `vi` is inverse Variance.

-   `atransf` will transform lnRR back to the **transformed ratio between the means(RR)**.

```{r myfunnel, echo=TRUE, fig.align='center', fig.cap= "Figure 2: Funnel plot depicting the transformed ratio(RR) between the means of control and treatment group as a function of precision (1 / SE). The dotted lines are the theoretical 95% sampling variance intervals - the interval with which we expect effect size estimates to fall within if only sampling variance drives differences in effects. Shaded regions represent the p-value of studies. The white region indicates studies where the p-value is between 0.1 and 1; dark gray where the p-value of studies is between 0.05 and 0.1 and the lighter gray regions where the p-value of studies is significant. Adjust the ylim and xlim manually and several out group values are excluded for a better look" }


metafor::funnel(x = ocean_total$lnRR, vi = ocean_total$V_lnRR, digits = 4, yaxis = "seinv", 
                level = c(0.1, 0.05, 0.01),
                shade = c("white", "gray55", "gray 75"), las = 1, 
                xlab = "The transformed ratio of means", legend = TRUE,ylim =c (0.1,30), atransf= exp,xlim = c(-3.5,3.5))
```

From Figure 2, I suspect there is no obvious file drawer effect.

The two sides of the data distribution is basically very uniform, there is no obvious blank area. The amount of data on both sides is almost equal.

**B. Plot of log transformed ratio of means against sampling variance for lnRR**

```{r, egger,fig.align='center', fig.cap= "Figure 3: Plot of log transformed ratio of means against sampling variance for lnRR. A linear model was fit to the data.",message=FALSE}
ggplot(ocean_total, aes(y = lnRR, x = V_lnRR)) + 
  geom_point() + 
  geom_smooth(method = lm) + 
  labs(y = "log transformed ratio of means", x = "Sampling Variance of lnRR") +
  theme_classic()
```

As we can see from Figure 3, there is not a significant slope coefficient. However, there are still unequal effect size distributions on both sides of the figure, and when the sampling variance is high, the average lnRR shifts, resulting in a slope that is not equal to 0. But in this study, the shift was not large. I concluded that no significant 'the file drawer effect' appeared in these studies.

It's not a big slope, but it still tends to be less than zero, indicating that most studies have an effect size (lnRR) smaller than 0. Shows a pattern that the mean in the treatment group is smaller than the control group in most studies.

#### 8. Time-lag plot assessing how effect sizes may or may not have changed through time

```{r, yearbubble,fig.align='center',fig.cap="Figure 4: Plot of lnRR as a function of publication year. Points are scaled in relation to their precision (1/sqrt(V_lnRR)). Small points indicate effects with low precision or high sampling varaince",warning=FALSE,message=FALSE}

ggplot(ocean_total, aes(y = lnRR, x = Year..online., size = 1/sqrt(V_lnRR))) + geom_point(alpha = 0.30) + geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year", y = "Log transformed ratio of means", size = "Precision (1/SE)") + theme_classic()

```

```{r}
num_less <- sum(1/sqrt(ocean_total$V_lnRR)>500)
```

**The key things which support suspicions about a weak time-lag bias being present in these data:**

1)  There does appear to be a weak negative relationship with year.

2)  There is not much difference between the precision in the early years and the precision in the recent years. There was even an earlier study that had the greatest precision, but it wasn't statistically significant.

3)  Earlier studies had some relatively large effect sizes, but they were few in number, and most effect sizes were closely distributed.

4)  There seems to be no significant relationship between the size of the variance and the year, and the variance in early years is not much larger than in recent years.

#### 9. Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias\*\*

First, add a column indicating the difference between each year and the average year.

```{r}
ocean_total <- ocean_total %>% mutate(Year_diff = Year..online. - mean(Year..online.))
```

Then build the model including mean centered year as moderators.

```{r timemodel, echo=TRUE,warning=FALSE}

MLMA_time <- metafor::rma.mv(lnRR ~ Year_diff, V = V_lnRR, 
                    random = list(~1|residual,
                                  ~1|Study), 
                    test = "t", dfs = "contain", 
                    data = ocean_total)
summary(MLMA_time)
```

```{r}
r2_time <- orchaRd::r2_ml(MLMA_time)
r2_time

```

#### 10. Formal meta-regression model that includes inverse sampling variance (1/v_lnRR) to test for file-drawer biases.

```{r,warning=FALSE,drawermodel}

MLMA_drawer <- metafor::rma.mv(lnRR~1/V_lnRR, V = V_lnRR, 
                   method="REML",
                   random=list(~1|Study,
                               ~1|residual), 
                   data=ocean_total)
MLMA_drawer
```

```{r}
r2_drawer <- orchaRd::r2_ml(MLMA_drawer)
i2_drawer <- tibble::tibble(type = orchaRd::firstup(gsub("I2_", "",names(r2_drawer))), I2 = r2_drawer)
r2_drawer[[1]]
```

Create a model that accounts for **both** of two effects(`sample variance` and `year`), and that accounts for the possible covariance between the two. And use `r2_ml()` functions to evaluate how much Heterogeneity does time when results were published explain in lnRR.

```{r timedrawermodel, echo=TRUE,warning = FALSE}

MLMA_both <- metafor::rma.mv(lnRR ~ Year_diff + 1/V_lnRR, V = V_lnRR, 
                    random=list(~1|Study,
                                ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = ocean_total)

summary(MLMA_both)
```

```{r}
r2_time_sv <- orchaRd::r2_ml(MLMA_both)
r2_time_sv
```

#### 11. A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

Two kinds of publication bias was test in previous work: **the file-drawer bias and time-lag bias.**

In the meta-regression model that includes inverse sampling variance (1/v_lnRR):

1.  The slope estimate for `V_lnRR` is not that significant. We can see from this model that the adjusted `lnRR` when there is *no* uncertainty (i.e., the intercept) is `r MLMA_drawer$b[1]` with a 95% confidence interval that overlaps zero (i.e., 95% CI = `r MLMA_drawer$ci.lb[1]` to `r MLMA_drawer$ci.ub[1]`). In other words, if no uncertainty around estimates exists, or we have a very high powered set of studies than we would expect the mean of effect size to be `r tanh(MLMA_drawer$b[1])`.

2.  We test that `r r2_drawer[[1]]`% of heterogeneity is produced by the inverse of sample variance. We can get to the conclusion that **no file-drawer bias is present in this study**.

In the meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias:

1.  The mean effect size is predicted to a very weak decrease as more studies accumulate.

2.  We test that `r r2_time[[1]]*100`% of heterogeneity is produced by the publication year. It is a very low rate of heterogeneity which indicate a very weak time-lag bias. Conditional $R^2$ shows that the full model, that accounts for the both the fixed and random effects, explains `r r2_time[2]*100`% of variance in effect size.

3.  There is a weak evidence of a time-lag bias. As time goes by, there is no significant difference between precision (1/sqrt(V_lnRR)) or the sample variance of different years. So we can draw the conclusion that **almost no** **time-lag bias is present in this study.**

I also create a model which including sampling variance and year as moderators. The overall mean lnRR (r) when small sample and time-lag biases are controlled for is `r MLMA_both$b[1]`. Which is not that different from the intercept in model only including sampling variance.

**So I get to the conclusion that the file-drawer bias and time-lag bias both not contribute to publication bias.**

#### 12. Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

**The differences in method:**

1.  First, only study and residual were considered as random effects in our study. However, Clement et. al. (2022) considered (1) cold-water species; (2) non-olfactory related behaviors; (3) Non-larval life stage and other influencing factors.

2.  Secondly, some very extreme lnRR variance values were deleted in our study, but Clement et. al. (2022) did not delete them, but included them into consideration.

3.  Then, Clement et. al. (2022) obtained the absolute value for lnRR, because many behavioral changes can be characterized by positive and negative functional trade-offs. But in my study, I kept the positive and negative values, so as to better observe the difference in mean value between the treatment group and the comparison group **and I wanted to know if ocean acidification had a positive or negative effect on behavior.**

**Concerns in meta-analysis:**

1.  First, in my meta-analysis, `I^2 total` = 100%, which indicates that there is no sampling variance. But considering the actual situation, I think this is not reliable.
2.  Secondly, in the meta-analysis, the size difference between the largest variance and the smallest variance is too large, which will lead to the instability of the meta-analysis modeling results.
3.  Thirdly, which is the most different thing, the file drawer bias and time-lag bias were not shown in my study, the deviation values were not very large and there was no obvious trend. However, Clement et. al. (2022) showed a clear decreasing trend

**Differences in meta-analysis results:**

1.  Clement et. al. (2022) discussed three aspects of bias in his study: (1) methodological bias; (2) selective publication bias; (3) citation bias. Our study only tested selective publication bias.

2.  Clement et. al. (2022) reveals an extreme "decline effect" in the impacts of ocean acidification on fish behavior. But after verified in terms of file-drawer bias and time-delay bias, we do not found a decreasing effect similar to the conclusion in the original paper. Both effect of file-drawer bias and time-delay bias are weak.
