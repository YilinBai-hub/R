---
title: "Assignment_2"
author: "Yilin Bai"
date: "2022-10-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### The link to my GitHub Repo

[My GitHub Repository(%5Bhttps://github.com/YilinBai-hub/R.git)](https://github.com/YilinBai-hub/R.git)

#### 1. Correct analysis of Clark data to generate the summary statistics (means, SD, N) for each of the fish species' average activity for each treatment.

**A. Libraries and Packages**

Install and load *tidyr, dplyr, plotrix, flextable, ggplot2, readr, orchaRd* package.

```{r}
library(tidyr)
library(dplyr)
library('plotrix')
library('flextable')
library(ggplot2)
library(readr)
library(orchaRd)
```

**B. Import of the Csv file**

Download the files "OA_activitydat_20190302_BIOL3207.csv" file from the course Wattle site. Import of the file into R as a tibble by using `read_csv()` and have a general impression of all the data.

```{r message=FALSE}
path <- "./Data/OA_activitydat_20190302_BIOL3207.csv"
data_activity <- read_csv(path)
data_activity
```

**C. Quality control of data to identify and address problematic entries**

There is some missing data in both `activity` and `animal_id` columns. Remove it with `filter(!is.na())`

```{r}
data_activity <- data_activity %>% filter(!is.na(activity))
data_activity <- data_activity %>% filter(!is.na(animal_id))
data_activity
```

**D. Generate the summary statistics**

Generate means, SD, N for each of the fish species' average activity for each treatment by using `group_by()` and `summarise()` function separately. And use the `merge()` function to combine the two tibbles of control and treatment.

```{r message=FALSE}
control_acticity <- data_activity %>% group_by(species) %>% filter(treatment=="control")%>% summarise(ctrl.n =n(),ctrl.mean = mean(activity),ctrl.sd = sd(activity))

co2_activity <-  data_activity %>% group_by(species) %>% filter(treatment=="CO2")%>% summarise(oa.n =n(),oa.mean = mean(activity),oa.sd = sd(activity))

group_activity <-  merge(control_acticity,co2_activity)
group_activity
```

#### 2. Through coding, merge the summary statistics generated from 1 with the metadata from Clark et al. (2020).

Import of the file into R as a tibble by using `read_csv()`.

Because there are no shared columns in the two tables, we use the `merge()` function to add the columns from both tables and generate a new table "clark_total".

```{r message=FALSE}
clark_paper_data <- read_csv("./Data/clark_paper_data.csv")
clark_total <- merge(clark_paper_data,group_activity)
clark_total
```

#### 3. Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset.

**A. Import of the file into R as a tibble by using `read_csv()` and have a general impression of all the data.**

```{r message=FALSE}
ocean_meta_data <- read_csv("./Data/ocean_meta_data.csv")
str(ocean_meta_data)
```

**B. Compare the two tibbles, there are some differences that may affect the merging of tables**

First, change the coercion of two column "Pub year IF" and "2017 IF" into character by using the function `as.character()`.

```{r}
clark_total$"Pub year IF" <- as.character(clark_total$"Pub year IF")
clark_total$"2017 IF" <- as.character(clark_total$"2017 IF")
```

Second, the "species" column names in the two tables are case-sensitive. Modify them to make them consistent.

```{r}
colnames(clark_total)[16] <- "Species" 
```

Third, I notice that in "ocean_meta_data2.csv", if there is no environmental Cue/stimulus, a "-" is used in the "Cue/stimulus type" column, whereas in "clark_total", The column "Cue/stimulus type" uses "NA", so change "NA" into "-".

```{r}
clark_total[is.na(clark_total)] <- "-"
```

**C. Merge two tables!**

```{r message=FALSE}
ocean_total <- bind_rows(clark_total,ocean_meta_data)
```

**D. Quality control of data to identify and address problematic entries**

According to the README file, the definition of activity column is **"Number of seconds the fish was active per minute, averaged across the duration of the trial."**

Therefore, the data value of the `activity` should be greater than 0, and the values of the `ctrl.mean` and `oa.mean` should also be greater than 0. Filter the data by using the `filter()` function.

```{r}
ocean_total <- ocean_total %>% filter(ctrl.mean>0) %>% filter(oa.mean>0)
```

#### 4. Correctly calculate the log response ratio (lnRR) effect size for every row of the data frame using metafor's `escalc()` function.

-   'ROM' - calculate the effect size (log response ratio of means) and sample variable of each row.

-   'm1i' and 'm2i' - the mean of treatment and control group

-   'sd1i' and 'sd2i' - the SD of treatment and control group. 'n1i' and 'n2i' - the sample size of treatment and control group.

-   Add the lnRR and sample variation as two columns named as 'lnRR', 'V_lnRR'.

```{r warning=FALSE}
ocean_total <- metafor::escalc(measure = "ROM", m1i = oa.mean, m2i = ctrl.mean, sd1i =oa.sd
, sd2i = ctrl.sd, n1i = oa.n, n2i = ctrl.n, data = ocean_total, var.names = c("lnRR", "V_lnRR"))
```

#### 5. Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor's `rma.mv()` function.

**A. Add in observation-level (`residual`) column.**

This column simply counts, from 1 to the number of rows. Add this to estimate the within study variation.

```{r}
ocean_total <- tibble::rowid_to_column(ocean_total, "residual")
```

**B. Filter out the NA values in effect size and sample variation which not contribute to the analyse.**

```{r}
ocean_total <- ocean_total %>% filter(!is.na(lnRR))
ocean_total <- ocean_total %>% filter(!is.na(V_lnRR))
```

**C. Filter out the data outliers**

-   Some sample variance are very large, which may be data outliers.

-   Because their sample variance is too large, it will lead to the instability of Multi-level meta-analytic model, which will also lead to difficulties in subsequent analysis and observation of publication bias.

-   So I filtered out the three outlier values from the data.

```{r}
ocean_total <- ocean_total %>% filter(V_lnRR<5000)
```

**D. Build the Multi-level meta-analytic model**

-   In this model,I define the effect size as `lnRR`, and the sample variance as `V_lnRR` by using the function `rma.mv()`.

-   I'm also estimating a random effect variance for `species`, between study (`study_id`) within-study (`residual`) grouping variables to control for non-independence and understand sources driving effect size variability.

```{r warning=FALSE}

# Multi-level meta-analytic model
MLMA <- metafor::rma.mv(lnRR~1, V = V_lnRR, 
                   method="REML",
                   random=list(~1|Study,
                               ~1|residual), 
                   data=ocean_total)
MLMA
```

Use the `predict()`function to transfer the lnRR(log respond ratio) back to RR (Respond ratio).

```{r,predict,fig.align='center',tab.cap = "The predict effect size and 95% confidence intervals of RR (Respond ratio)" }
pis <- predict(MLMA,transf=exp)
pis
```

**E. Use `i2_ml()`function to estimate the proportion of Total Heterogeneity:** $I_{total}^2$

```{r}
i2_vals <- orchaRd::i2_ml(MLMA)
i2 <- tibble(type = firstup(gsub("I2_", "",names(i2_vals))), I2 = i2_vals)
```

```{r, het}

flextable(i2) %>% 
    align(part = "header", align = "center") %>% 
  compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>% 
  compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")), as_b("(%)")))
```

#### 6. Written paragraph of the findings and what they mean which is supported with a figure.

**A. Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).**

I change the measure lnRR back to RR(respond ratio)

-   Overall meta-analytic mean - Estimated to be `r pis$pred`, which tells us that the mean RR value is larger than 1, indicate the average value of the control group is little higher than that of the treatment group. But it is a weak difference.

-   95% confidence intervals: `r pis[[3]]` to `r pis[[4]]`. We have 95% confident true mean of the RR value falls between `r pis[[3]]` and `pis[[4]]`.

-   NULL hypothesis - After test, it is found that there is no significant difference between the mean of two groups( p value = `r MLMA$pval` \> 0.05). So we accept the NULL hypothesis (lnRR = 0). Indicate that increase of acidification do not have significant effects on behavior.

**B. Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor)**

-   The total I2 = `r i2_vals[[1]]` indicates a significant amount of heterogeneity among effects (Q = 3941, df = 122, p = \<0.001). Whereas none of the variation is driven by the sampling variance and there is lot of heterogeneity in the data.

-   The **RR** expected to be as low as `r predict(MLMA)[[5]]` to as high as `r predict(MLMA)[[6]]`95% of the time.

-   The factors drives heterogeneity variability: Differences among studies explain `r i2_vals[[2]]`% of effect size variation. And the difference within the study explain `r i2_vals[[3]]`% of the effect size variation.

**C. Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure**

```{r rochard, fig.align='center', fig.cap= "Figure 1: Forest plot showing the mean lnRR for the log response ratio between the treatment group and control group. k = the number of effect sizes and the number of studies are in brackets. The size of the effect is scaled by the precision of each effect size value, which is 1 / the sqrt(v_lnRR)"}

orchaRd::orchard_plot(MLMA, group = "Study", data = ocean_total, xlab = "The log response ratio(lnRR)", angle = 45,trunk.size = 2,branch.size = 10,twig.size = 2)
```

#### 7. Funnel plot for visually assessing the possibility of publication bias.

**A. Generate the Funnel plot with the function `funnel()`.**

-   `x` is the vector of effect sizes, lnRR.

-   `vi` is inverse Variance.

-   `atransf` will transform lnRR back to the **transformed ratio between the means**.

```{r funnel, echo=TRUE, fig.align='center', fig.cap= "Figure 2: Funnel plot depicting the transformed ratio(RR) between the means of control and treatment group as a function of precision (1 / SE). The dotted lines are the theoretical 95% sampling variance intervals - the interval with which we expect effect size estimates to fall within if only sampling variance drives differences in effects. Shaded regions represent the p-value of studies. The white region indicates studies where the p-value is between 0.1 and 1; dark gray where the p-value of studies is between 0.05 and 0.1 and the lighter gray regions where the p-value of studies is significant." }

metafor::funnel(x = ocean_total$lnRR, vi = ocean_total$V_lnRR, digits = 4, yaxis = "seinv", 
                level = c(0.1, 0.05, 0.01),
                shade = c("white", "gray55", "gray 75"), las = 1, 
                xlab = "The transformed ratio of means", legend = TRUE,ylim =c (0.1,30), atransf= exp,xlim = c(-3.5,3.5))
```

There is a few messages we can get from the forest plot:

-   From Figure above, the most effects lie space which **The transformed ratio of means was less than 1.** So, the mean of control group is larger than treatment group in most studies.

-   There is still some studies that show the opposite pattern which the **transformed ratio of means was larger than 1**. I expect that based on sampling theory alone, and indeed many of these effects fall close to the dotted sampling error intervals. Studies in the light grey regions are studies where the p-value was significant.

-   There is a noticeable blank space in the bottom right corner with the **lnRR larger than 1** These studies have opposite results, they are not sure of the correctness and authenticity of the research because the sample is too small and the sample variance is too large. They also failed to find a significant difference between the mean of treatment and control group. So these studies are not being published. Just like the **file-drawer situation**, these drive what we call funnel asymmetry, showing a bunch of missing effect sizes in the bottom left corner of the funnel.

-   But also if the magnitude of RR is large enough in the opposite direction (RR\>1) even with small sample sizes these can get published, which has a p value less than 0.05. These arguably "surprising" results are more likely to be published if the difference in the mean between the two treatments is large enough, and in the opposite direction of what one would expect, than if the difference is small and in opposite direction.

**B. Plot of log transformed ratio of means against sampling variance for lnRR**

```{r, egger,fig.align='center', fig.cap= "Figure 3: Plot of log transformed ratio of means against sampling variance for lnRR. A linear model was fit to the data.",message=FALSE}
ggplot(ocean_total, aes(y = lnRR, x = V_lnRR)) + 
  geom_point() + 
  geom_smooth(method = lm) + 
  labs(y = "log transformed ratio of means", x = "Sampling Variance of lnRR") +
  theme_classic()
```

There is a significant slope coefficient. That's because there is an unequal distribution of effect sizes on either side of the plot and the mean lnRR when the sampling variance is high gets shifted resulting in the slope being different from 0.

The slope coefficient is less than 0, indicating that most studies have an effect size smaller than 0. Shows a pattern that the mean in the treatment group is smaller than the control group.

#### 8. Time-lag plot assessing how effect sizes may or may not have changed through time

```{r, yearbubble,fig.align='center',fig.cap="Figure 4: Plot of lnRR as a function of publication year. Points are scaled in relation to their precision (1/sqrt(V_lnRR)). Small points indicate effects with low precision or high sampling varaince",warning=FALSE,message=FALSE}

ggplot(ocean_total, aes(y = lnRR, x = Year..online., size = 1/sqrt(V_lnRR))) + geom_point(alpha = 0.30) + geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year", y = "Log transformed ratio of means", size = "Precision (1/SE)") + theme_classic()

```

```{r}
num_less <- sum(1/sqrt(ocean_total$V_lnRR)>500)
```

**There are a few key things which support our suspicions about time-lag bias being present in these data:**

1)  There does appear to be a clear negative relationship with year.

2)  The earlier year studies have much higher sampling variance. Because the sample size is much more smaller, indicate a larger sampling variance and lower precision.

3)  These early studies appear to have a far higher (exaggerated) effect size compared with studies that are done in later years.

But in this figure, the difference between the precision is hard to tell. I found that there are only `r num_less` study with a `1/SE` larger than 0, but these precision are extremely large, infecting the precision visualization of other studies. So I try to filter out these five data to observe better the relationship between effect size and year in the remaining studies.

```{r, yearbubblemod,fig.align='center',fig.cap="Figure 5: A Plot of lnRR filtered out data with tremendous precision as a function of publication year. Points are scaled about their precision (1/sqrt(V_lnRR)). Small points indicate effects with low precision or high sampling variance. After being manually modified, this figure can intuitively see the precision change over the years.",warning=FALSE,message=FALSE}

ocean_total_test <- ocean_total %>% filter(1/sqrt(V_lnRR)<500)

ggplot(ocean_total_test, aes(y = lnRR, x = Year..online., size = 1/sqrt(V_lnRR))) + geom_point(alpha = 0.30) + geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year", y = "Log transformed ratio of means", size = "Precision (1/SE)") + theme_classic()
```

#### 9. Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias**

First, add a column indicating the difference between each year and the average year.

```{r}
ocean_total <- ocean_total %>% mutate(Year_diff = Year..online. - mean(Year..online.))
```

Then build the model including mean centered year as moderators.

```{r timemodel, echo=TRUE,warning=FALSE}

MLMA_time <- metafor::rma.mv(lnRR ~ Year_diff, V = V_lnRR, 
                    random = list(~1|residual,
                                  ~1|Study), 
                    test = "t", dfs = "contain", 
                    data = ocean_total)
summary(MLMA_time)

r2_time <- orchaRd::r2_ml(MLMA_time)
r2_time
```

#### 10. Formal meta-regression model that includes inverse sampling variance (1/v_lnRR) to test for file-drawer biases.

```{r,warning=FALSE,drawermodel}

MLMA_drawer <- metafor::rma.mv(lnRR~1/V_lnRR, V = V_lnRR, 
                   method="REML",
                   random=list(~1|Study,
                               ~1|residual), 
                   data=ocean_total)
MLMA_drawer

r2_drawer <- orchaRd::r2_ml(MLMA_drawer)

i2_drawer <- tibble(type = firstup(gsub("I2_", "",names(r2_drawer))), I2 = r2_drawer)
i2_drawer

```

Create a model that accounts for **both** of two effects(`sample variance` and `year`), and that accounts for the possible covariance between the two. And use `r2_ml()` functions to evaluate how much Heterogeneity does time when results were published explain in lnRR.

```{r timedrawermodel, echo=TRUE,warning = FALSE}

MLMA_both <- metafor::rma.mv(lnRR ~ Year_diff + 1/V_lnRR, V = V_lnRR, 
                    random=list(~1|Study,
                                ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = ocean_total)


summary(MLMA_both)

r2_time_sv <- orchaRd::r2_ml(MLMA_both)
r2_time_sv
```

#### 11. A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

First, I expect under a file-drawer bias. Researchers will not published the poorer quality studies showing opposite effects. Include studies with low power (i.e., low precision, wide standard errors, and small sample sizes) and non-significant differences between treatment group and control group.

The evidence for file-drawer bias in data:

The slope estimate for `V_lnRR` is significant. We can see from this model that the adjusted `lnRR` when there is *no* uncertainty (i.e., the intercept) is `r MLMA_drawer$b[1]` with a 95% confidence interval that overlaps zero (i.e., 95% CI = `r MLMA_drawer$ci.lb[1]` to `r MLMA_drawer$ci.ub[1]`). In other words, if no uncertainty around estimates exists, or we have a very high powered set of studies than we would expect the mean of effect size to be `r tanh(MLMA_drawer$b[1])`.

Second, it's clear that we have evidence of a time-lag bias.As time goes by, the precision (1/sqrt(V_lnRR)) of the obtained data will be higher and higher and the variance will be smaller and smaller due to the larger sample size. The distribution of effect size will also narrow.

The evidence for time-lag bias in data: 1. The mean effect size is predicted to decrease as more studies accumulate. 2. Time-lag explains `r r2_time[1]`% of the variation in lnRR. 3. The intercept when `year_diff` = 0 is `r tanh(MLMA_both$b[1])`. We can see that sampling variance explains `r r2_time[1]*100`% of effect size variance. Conditional $R^2$ shows that the full model, that accounts for the both the fixed and random effects, explains `r r2_time[2]*100`% of variance in effect size.

I also create a model which including sampling variance and year as moderators. The overall mean lnRR (r) when small sample and time-lag biases are controlled for is `r MLMA_both$b[1]`. Which is different from the intercept in model only including sampling variance. So the sampling variance and year both contribute to publication bias.

#### 12. Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

**The differences in method:**

1.  First, only study and residual were considered as random effects in our study. However, Clement et. al. (2022) considered (1) cold-water species; (2) non-olfactory related behaviors; (3) Non-larval life stage and other influencing factors.

2.  Secondly, some very extreme lnRR variance values were deleted in our study, but Clement et. al. (2022) did not delete them, but included them into consideration.

3.  Then, Clement et. al. (2022) obtained the absolute value for lnRR, because many behavioral changes can be characterized by positive and negative functional trade-offs. But in my study, I kept the positive and negative values, so as to better observe the difference in mean value between the treatment group and the comparison group **and I wanted to know if ocean acidification had a positive or negative effect on behavior.**

**Concerns in meta-analysis:**

1.  First, in my meta-analysis, `I^2 total` = 100%, which indicates that there is no sampling variance. But considering the actual situation, I think this is not real.
2.  Secondly, in the meta-analysis, the size difference between the largest variance and the smallest variance is too large, which will lead to the instability of the meta-analysis modeling results.
3.  Thirdly, although file drawer bias and time-lag bias were shown, the deviation values were not very large and there was no obvious trend. However, Clement et. al. (2022) showed a clear decreasing trend.

**Differences in meta-analysis results:**

Clement et. al. (2022) discussed three aspects of bias in his study: (1) methodological bias; (2) selective publication bias; (3) citation bias. Our study only tested selective publication bias. We verified in terms of file-drawer bias and time-delay bias, and found a decreasing effect similar to the conclusion in the original paper.
